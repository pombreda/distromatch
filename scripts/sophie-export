#!/usr/bin/python

# Export distromatcher input information from Sophie.
# See: http://www.enricozini.org/2011/debian/distromatch/

__author__ = "Enrico Zini <enrico@debian.org>"
__license__ = """
    Copyright (C) 2011 Enrico Zini <enrico@debian.org>

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License along
    with this program; if not, write to the Free Software Foundation, Inc.,
    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
"""

import psycopg2
import logging
import sys
import os
import os.path
from gzip import GzipFile

# FIXME: I hate messing with sys.path
sys.path.append("..")
import dmatch.rules as rules

log = logging.getLogger(__name__)

class Extractor(object):
    def __init__(self, outdir="."):
        self.outdir = outdir
        try:
            # FIXME: temporary DB connection info
            #self.db = psycopg2.connect(host="localhost",port=5432,user="nobody",database="sophie")
            self.db = psycopg2.connect(host="sophie-db.latmos.ipsl.fr",port=5432,user="nobody",database="sophie")
        except Exception, e:
            log.warn("cannot connect to sophie: %s", str(e))
            sys.exit(1)

    def query_binsrc(self, tag, rk):
        c = self.db.cursor("binsrc")
        c.execute("""
        SELECT Rpmfiles.filename, Rpmtags.value
          FROM d_arch Arch
          JOIN d_media Medias ON Medias.d_arch = Arch.d_arch_key
          JOIN d_media_path MediasPaths ON MediasPaths.d_media = Medias.d_media_key
          JOIN d_path Paths ON Paths.d_path_key = MediasPaths.d_path
          JOIN rpmfiles Rpmfiles ON Rpmfiles.d_path = Paths.d_path_key
          JOIN rpms_tags Rpmtags ON Rpmtags.pkgid = Rpmfiles.pkgid
         WHERE Arch.d_release = %s AND Rpmtags.tagname = 'sourcerpm'
        """, (rk,))
        count = 0
        for fn, sfn in c:
            binname = fn.rsplit("-", 3)[0]
            srcname = sfn.rsplit("-", 2)[0]
            yield binname, srcname
            count += 1
            if count % 10000 == 0:
                log.info("%s:%dk bin<->src mappings read", tag, count / 1)
            #if count == 2000:
            #    log.info("debugging break")
            #    break
        c.close()

    def query_files(self, tag, rk):
        c = self.db.cursor("files")
        c.execute("""
        SELECT Rpmfiles.filename, Directories.directory, Binfiles.basename
          FROM d_arch Arch
          JOIN d_media Medias ON Medias.d_arch = Arch.d_arch_key
          JOIN d_media_path MediasPaths ON MediasPaths.d_media = Medias.d_media_key
          JOIN d_path Paths ON Paths.d_path_key = MediasPaths.d_path
          JOIN rpmfiles Rpmfiles ON Rpmfiles.d_path = Paths.d_path_key
          JOIN binfiles Binfiles ON Binfiles.pkgid = Rpmfiles.pkgid
          JOIN directories Directories on Directories.dir_key = binfiles.dirnamekey
         WHERE Arch.d_release = %s
        """, (rk,))
        count_read = 0
        count_matched = 0
        for rpmfile, dirname, basename in c:
            binname = rpmfile.rsplit("-", 3)[0]
            fname = os.path.join(dirname, basename)

            # Select content
            for kind, matcher in rules.CONTENT_INFO.iteritems():
                m = matcher.match(fname)
                if m:
                    yield binname, kind, fname
                    count_matched += 1

            count_read += 1
            if count_read % 100000 == 0:
                log.info("%s:%dk paths read, %d paths matched", tag, count_read/1000, count_matched)
            #if count_read == 100:
            #    log.info("debugging break")
            #    break
        c.close()

    def writeout(self, fname, items):
        """
        Atomically write the items generated by @items to their target file
        """
        out = GzipFile(fname+".tmp", "w")
        for i in items:
            print >>out, " ".join(i)
        out.close()
        os.rename(fname+".tmp", fname)

    def do_all(self):
        # Read the list of (distro, release) we are exporting
        c = self.db.cursor()
        c.execute("""
        SELECT d.name, r.version, r.d_release_key
          FROM distributions d, d_release r
         WHERE r.distributions = d.distributions_key
        """)
        for name, ver, rk in c:
            # Build the tag we use for file names
            tname = name.lower().replace(" ", "-")
            tver = ver.lower().replace(" ", "-")
            tag = tname + "-" + tver

            log.info("%s:%s %s", tag, name, ver)

            # Export binsrc
            log.info("%s:exporting binary<->source package mapping", tag)
            fname = os.path.join(self.outdir, "binsrc-" + tag + ".gz")
            self.writeout(fname, self.query_binsrc(tag, rk))

            # Export files
            log.info("%s:exporting filtered file list", tag)
            fname = os.path.join(self.outdir, "interesting-files-" + tag + ".gz")
            self.writeout(fname, self.query_files(tag, rk))

    def query_all_files(self):
        count_read = 0
        count_matched = 0

        # Build the file extract query, merging the SQL directory constraints
        # in CONTENT_INFO
        sql_eq = set()
        sql_like = set()
        for kind, matcher in rules.CONTENT_INFO.iteritems():
            cons = matcher.sophie
            if cons is None: continue
            sql_eq.update(cons.get("eq", []))
            sql_like.update(cons.get("like", []))
        # No big risk of SQL injection here as the patterns come from a trusted
        # source
        query = """
        CREATE TEMP TABLE wanted_dirs AS
        SELECT d.dir_key, d.directory
          FROM directories d
         WHERE d.directory IN (%s)
            OR %s
        """ % (
            ", ".join(["'%s'" % x for x in sql_eq]),
            " OR ".join(["d.directory LIKE '%s'" % x for x in sql_like])
        )
        log.info("Dir shopping list query: %s", query)
        temp_table = self.db.cursor()
        temp_table.execute(query)
        log.info("query-all-files:built directory shopping list in wanted_dirs")

        query = """
        SELECT f.pkgid, d.directory, f.basename
          FROM binfiles f
          JOIN wanted_dirs d on d.dir_key = f.dirnamekey;
        """
        c = self.db.cursor("files")
        c.execute(query)
        batch_size = max(c.arraysize, 2048)
        log.info("query-all-files:psycopg2 suggests batch size of %d; using %d", c.arraysize, batch_size)
        while True:
            rows = c.fetchmany(batch_size)
            if not rows: break
            for pkgid, dirname, basename in rows:
                fname = os.path.join(dirname, basename)
                count_read += 1
                for kind, matcher in rules.CONTENT_INFO.iteritems():
                    # Further filtering by regexp
                    m = matcher.match(fname)
                    if m:
                        yield pkgid, kind, fname
                        count_matched += 1
                if count_read % 100000 == 0:
                    log.info("query-all-files:%dk paths read, %d paths matched", count_read/1000, count_matched)
        c.close()
        temp_table.close()

    def do_files(self):
        # Export files
        log.info("exporting full filtered file list")
        fname = os.path.join(self.outdir, "all-interesting-files.gz")
        self.writeout(fname, self.query_all_files())

if __name__ == "__main__":
    date_format = "%c"
    log_format = "%(asctime)s:%(levelname)s:%(name)s:%(message)s"
    if os.isatty(sys.stderr.fileno()):
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, datefmt=date_format, format=log_format)
        #logging.basicConfig(level=logging.DEBUG, stream=sys.stderr)
    else:
        logging.basicConfig(level=logging.WARNING, stream=sys.stderr, datefmt=date_format, format=log_format)

    # FIXME: temporary output dir
    extractor = Extractor(".")
    #extractor.do_all()
    extractor.do_files()

