#!/usr/bin/python

# distromatch - Match binary package names across distributions
#
# Copyright (C) 2011  Enrico Zini <enrico@enricozini.org>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Update all Debian distromatch sources

import sys
import os
import os.path
import re
import time
import logging
import psycopg2
import gzip
import urllib
import calendar
import email.utils
import subprocess
import cPickle as pickle
from cStringIO import StringIO

log = logging.getLogger(__name__)

class Binsrc(object):
    def __init__(self):
        self.db = psycopg2.connect(host="localhost",port=5441,user="guest",database="udd")

    def get(self, dist=None, release=None):
        """
        Generate (bin, src) couples with binary-source package mappings for the
        given distribution and release
        """
        c = self.db.cursor()

        if dist is None:
            c.execute("SELECT DISTINCT package, source FROM all_packages"
                      " ORDER BY package, source")
        elif release is None:
            c.execute("SELECT DISTINCT package, source FROM all_packages"
                      " WHERE distribution=%s"
                      " ORDER BY package, source", (dist,))
        else:
            c.execute("SELECT DISTINCT package, source FROM all_packages"
                      " WHERE distribution=%s and release=%s"
                      " ORDER BY package, source", (dist, release))

        for pkg, src in c:
            yield pkg, src

class IntFiles(object):
    def __init__(self, cachedir):
        self.cachedir = cachedir

    def cached(self, url, processor):
        """
        Run @processor on the contents of @url, caching the results and reusing the
        cached results if the url contents have not changed
        """
        zfd = urllib.urlopen(url)

        # Get the file timestamp
        # FIXME: timezone conversion may happen here, since parsedate documentation
        # doesn't mention timezones
        ts = calendar.timegm(email.utils.parsedate(zfd.info()["Last-modified"]))

        # Check if we have the data in cache
        name = re.sub(r"[^a-zA-Z0-9_.-]", "_", url)
        cachefile = os.path.join(self.cachedir, name)
        try:
            cachefile_ts = os.path.getmtime(cachefile)
        except Exception, e:
            cachefile_ts = 0
        if cachefile_ts >= ts:
            # Read from cache
            log.info("%s: read from cache", url)
            return pickle.load(gzip.GzipFile(cachefile, "r"))

        # Not found in cache: read the real data
        log.info("%s: recomputing", url)

        # Open through gunzipper
        # FIXME: see http://www.enricozini.org/2011/cazzeggio/python-gzip/
        #        for why there is such a stupid thing as a StringIO(zfd.read())
        #        here.
        fd = gzip.GzipFile(mode="r", fileobj=StringIO(zfd.read()))

        # Run the processor
        res = processor(fd)

        # Save the processed data in cache
        out = gzip.GzipFile(cachefile, mode="w")
        pickle.dump(res, out, pickle.HIGHEST_PROTOCOL)
        out.close()
        # Set file timestamp to ts
        os.utime(cachefile, (ts, ts))

        # Return the results
        return res

    def contentfiles(self, srcdir):
        fname = os.path.join(srcdir, "contentfiles")
        if not os.path.exists(fname):
            return

        if os.access(fname, os.X_OK):
            proc = subprocess.Popen([fname], stdout=subprocess.PIPE)
            for line in proc.stdout:
                yield line.strip()
            if proc.wait() != 0:
                raise RuntimeError("%s exited with %d error code",
                                   fname, proc.returncode)
        else:
            with open(fname, "r") as fd:
                for line in fd:
                    yield line.strip()

    def parse_contents(self, fd):
        "Generate pkg,file couples from a Debian Contents-$ARCH file"
        for line in fd:
            if line.startswith("FILE"): break
        for line in fd:
            line = line[:-1]
            try:
                file, pkg = line.rsplit(None, 1)
                pkg = pkg.rsplit('/', 1)[1]
            except:
                continue
            yield pkg, file

    def read_contents(self, fd):
        """
        Read Contents-$ARCH data from the given file descriptor
        and return its contents into a set((binpkgname, kind, path))
        """
        count_read = 0
        count_matched = 0
        tgt = set()
        for pkg, fname in self.parse_contents(fd):
            # Select content
            for kind, matcher in dmatch.CONTENT_INFO.iteritems():
                m = matcher.match(fname)
                if m:
                    tgt.add((pkg, kind, m))
                    count_matched += 1
            count_read += 1
            if count_read % 1000000 == 0:
                log.info("%s:%dk paths read, %d paths matched", url, count_read/1000, count_matched)
        return tgt



if __name__ == "__main__":
    from optparse import OptionParser

    VERSION="0.1"

    class Parser(OptionParser):
        def error(self, msg):
            sys.stderr.write("%s: error: %s\n\n" % (self.get_prog_name(), msg))
            self.print_help(sys.stderr)
            sys.exit(2)

    scriptdir = os.path.dirname(sys.argv[0])

    # FIXME: I hate messing with sys.path
    sys.path.append(os.path.abspath(os.path.join(scriptdir, "..")))
    import dmatch
    from dmatch import utils

    # Compute config defaults
    datadir = os.path.abspath(os.path.join(scriptdir, "..", "data"))
    cachedir = os.path.abspath(os.path.join(scriptdir, "..", "cache"))

    parser = Parser(usage="usage: %prog [options]",
                    version="%prog "+ VERSION,
                    description="Update all Debian distromatch sources")
    parser.add_option("-q", "--quiet", action="store_true",
                      help="quiet mode: only output warnings and errors")
    parser.add_option("--datadir", action="store", metavar="dir", default=datadir,
                      help="pathname to the data directory, where dist-* dirs are located"
                           " (default: %default)")
    parser.add_option("--cachedir", action="store", metavar="dir", default=cachedir,
                      help="pathname to the cache directory, intermediate results are stored"
                           " between update runs (default: %default)")
    (opts, args) = parser.parse_args()

    FORMAT = "%(asctime)-15s %(levelname)s %(message)s"
    if opts.quiet:
        logging.basicConfig(level=logging.WARNING, stream=sys.stderr, format=FORMAT)
    else:
        logging.basicConfig(level=logging.INFO, stream=sys.stderr, format=FORMAT)

    # TODO export CACHEDIR

    if not os.path.isdir(opts.cachedir):
        os.makedirs(opts.cachedir)

    # TODO touch -d "2 days ago" $CACHEDIR/rebuild-threshold
    now = time.time()

    binsrc = Binsrc()
    intfiles = IntFiles(opts.cachedir)

    for d in os.listdir(opts.datadir):
        mo = re.match(r"^dist-(debian|ubuntu)-(.+)", d)
        if not mo: continue
        dist = mo.group(1)
        release = mo.group(2)

        sourcedir = os.path.join(opts.datadir, d)

        # Get the time this source was last updated
        fn_last_update = os.path.join(sourcedir, "last_update")
        if os.path.exists(fn_last_update):
            lastrun = os.path.getmtime(fn_last_update)
        else:
            lastrun = 0

        # Skip if it is quite up to date
        if now - lastrun < 86400 * 2:
            log.info("Skipping %s, which is only %.1fh old", d, (now - lastrun)/3600.0)
            continue

        log.info("Building binary<->sources mappings for %s %s (reading from UDD)", dist, release)

        with utils.atomic_writer(os.path.join(sourcedir, "binsrc.gz")) as fd:
            gzfd = gzip.GzipFile("binsrc", "w", fileobj=fd)
            for bin, src in binsrc.get(dist, release):
                print >>gzfd, bin, src
            gzfd.close()

        log.info("Building interesting-files list for %s %s", dist, release)

        # Merged pkg->set(files) dist
        merged = set()

        # Read and merge all info
        for url in intfiles.contentfiles(sourcedir):
            info = intfiles.cached(url, intfiles.read_contents)
            merged |= info

        # Save the merged lists
        with utils.atomic_writer(os.path.join(sourcedir, "interesting-files.gz")) as fd:
            gzfd = gzip.GzipFile("interesting-files", "w", fileobj=fd)
            for pkg, kind, path in sorted(merged):
                print >>gzfd, pkg, kind, path
            gzfd.close()

        # Touch the last_update file to indicate when we ran
        with open(fn_last_update, "w"):
            pass
